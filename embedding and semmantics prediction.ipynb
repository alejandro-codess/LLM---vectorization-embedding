{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a file with 4 \".txt\" type of files, and analysis through its contect and feature prediction by using semmantics, embedding and vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of commas: 16\n",
      "Number of capital letters: 49\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Path to your text file\n",
    "file_path = r\"C:/Users/u651278/Downloads/text files/pc_jabberwocky.txt\"  # Update with the correct path\n",
    "\n",
    "# Initialize counters\n",
    "comma_count = 0\n",
    "capital_letter_count = 0\n",
    "\n",
    "# Read the file and count commas and capital letters\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "        # Count commas\n",
    "        comma_count = content.count(',')\n",
    "        \n",
    "        # Count capital letters using regular expressions\n",
    "        capital_letter_count = len(re.findall(r'[A-Z]', content))\n",
    "        \n",
    "    print(f\"Number of commas: {comma_count}\")\n",
    "    print(f\"Number of capital letters: {capital_letter_count}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {file_path} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the output of commas and capitals for one of the files, which we care about to see if the system can work with them for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: C:\\Users\\YourName\\Documents\\text files\\your_file.txt not found.\n",
      "Accuracy: 0.9369\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       208\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.94       222\n",
      "   macro avg       0.31      0.33      0.32       222\n",
      "weighted avg       0.88      0.94      0.91       222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Path to your text file\n",
    "file_path = r\"C:\\Users\\YourName\\Documents\\text files\\your_file.txt\"  # Update with the correct path\n",
    "\n",
    "# Read the content of the file\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {file_path} not found.\")\n",
    "    exit()\n",
    "\n",
    "# Prepare the features and labels\n",
    "window_size = 5  # Number of previous characters used to predict the next one\n",
    "\n",
    "X = []  # Features (text snippets)\n",
    "y = []  # Labels (comma or capital letter)\n",
    "\n",
    "# Function to check if a character is a comma or a capital letter\n",
    "def is_comma_or_capital(char):\n",
    "    if char == ',':\n",
    "        return 1  # Comma\n",
    "    elif char.isupper():\n",
    "        return 2  # Capital letter\n",
    "    return 0  # Neither comma nor capital letter\n",
    "\n",
    "# Loop through the content and prepare data\n",
    "for i in range(len(content) - window_size):\n",
    "    # Create a feature vector for the previous 'window_size' characters\n",
    "    window = content[i:i + window_size]\n",
    "    \n",
    "    # Create the target variable: what comes next (comma or capital letter)\n",
    "    next_char = content[i + window_size]\n",
    "    \n",
    "    # Add the feature (text snippet)\n",
    "    X.append(window)  # Use text snippets as features\n",
    "    \n",
    "    # Add the label for the next character (comma or capital letter)\n",
    "    y.append(is_comma_or_capital(next_char))\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Use TF-IDF to vectorize the text snippets (features)\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))  # Using character bigrams and trigrams\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By embbeding and vectorizing our text file, the system was propperly trained and achieved a .9369 accuracy for commas and capital letters prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9369\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       208\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.94       222\n",
      "   macro avg       0.31      0.33      0.32       222\n",
      "weighted avg       0.88      0.94      0.91       222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report\n",
    "\n",
    "# Path to your text file\n",
    "file_path = r\"C:/Users/u651278/Downloads/text files/pc_jabberwocky.txt\"  # Update with the correct path\n",
    "\n",
    "# Read the content of the file\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {file_path} not found.\")\n",
    "    exit()\n",
    "\n",
    "# Prepare the features and labels\n",
    "window_size = 5  # Number of previous characters used to predict the next one\n",
    "\n",
    "X = []  # Features (text snippets)\n",
    "y = []  # Labels (comma or capital letter)\n",
    "\n",
    "# Function to check if a character is a comma or a capital letter\n",
    "def is_comma_or_capital(char):\n",
    "    if char == ',':\n",
    "        return 1  # Comma\n",
    "    elif char.isupper():\n",
    "        return 2  # Capital letter\n",
    "    return 0  # Neither comma nor capital letter\n",
    "\n",
    "# Loop through the content and prepare data\n",
    "for i in range(len(content) - window_size):\n",
    "    # Create a feature vector for the previous 'window_size' characters\n",
    "    window = content[i:i + window_size]\n",
    "    \n",
    "    # Create the target variable: what comes next (comma or capital letter)\n",
    "    next_char = content[i + window_size]\n",
    "    \n",
    "    # Add the feature (text snippet)\n",
    "    X.append(window)  # Use text snippets as features\n",
    "    \n",
    "    # Add the label for the next character (comma or capital letter)\n",
    "    y.append(is_comma_or_capital(next_char))\n",
    "\n",
    "# Convert to pandas DataFrame for easier manipulation\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Use TF-IDF to vectorize the text snippets (features)\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))  # Using character bigrams and trigrams\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', labels=[1, 2])  # Comma (1) and Capital letter (2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9476\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       578\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       1.00      0.04      0.07        26\n",
      "\n",
      "    accuracy                           0.95       611\n",
      "   macro avg       0.65      0.35      0.35       611\n",
      "weighted avg       0.94      0.95      0.92       611\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3-2024.06\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Folder is already open in VSCode, so use the relative path\n",
    "folder_path = \"C:/Users/u651278/Downloads/text files\"  # Use relative path if folder is within the workspace\n",
    "\n",
    "def read_text_files_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # Only consider .txt files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Use 'ignore' or 'replace' to handle invalid characters\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    texts.append(file.read())  # Add the content of each file\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {file_path} not found.\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Read the content of all text files in the folder\n",
    "texts = read_text_files_from_folder(folder_path)\n",
    "\n",
    "# Prepare the features and labels\n",
    "window_size = 5  # Number of previous characters used to predict the next one\n",
    "\n",
    "X = []  # Features (text snippets)\n",
    "y = []  # Labels (comma or capital letter)\n",
    "\n",
    "# Function to check if a character is a comma or a capital letter\n",
    "def is_comma_or_capital(char):\n",
    "    if char == ',':\n",
    "        return 1  # Comma\n",
    "    elif char.isupper():\n",
    "        return 2  # Capital letter\n",
    "    return 0  # Neither comma nor capital letter\n",
    "\n",
    "# Loop through the content of all files and prepare data\n",
    "for text in texts:\n",
    "    for i in range(len(text) - window_size):\n",
    "        # Create a feature vector for the previous 'window_size' characters\n",
    "        window = text[i:i + window_size]\n",
    "        \n",
    "        # Create the target variable: what comes next (comma or capital letter)\n",
    "        next_char = text[i + window_size]\n",
    "        \n",
    "        # Add the feature (text snippet)\n",
    "        X.append(window)  # Use text snippets as features\n",
    "        \n",
    "        # Add the label for the next character (comma or capital letter)\n",
    "        y.append(is_comma_or_capital(next_char))\n",
    "\n",
    "# Convert to numpy arrays for model compatibility\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Use TF-IDF to vectorize the text snippets (features)\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))  # Using character bigrams and trigrams\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the same idea for processing languages in the system and the accuracy raises from .9369 to .9476 meaning we are heading in the correct direction to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One fo the files has manny special characters, cleaning and post rerunning of the model will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0 (from clean-text)\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "     ---------------------------------------- 0.0/175.4 kB ? eta -:--:--\n",
      "     -- ------------------------------------- 10.2/175.4 kB ? eta -:--:--\n",
      "     -------------------- ------------------ 92.2/175.4 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 175.4/175.4 kB 3.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy<7.0,>=6.0 (from clean-text)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: wcwidth in c:\\anaconda3-2024.06\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
      "Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.8/44.8 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171059 sha256=23e273e19bdc4b415cec95db98d41662a9b2b7b535d935ef91886cf9caf42aa5\n",
      "  Stored in directory: c:\\users\\u651278\\appdata\\local\\pip\\cache\\wheels\\e0\\8c\\e0\\294d2e4ea0e55792bfc99b6b263e4a0511443da7b69af67688\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, ftfy, clean-text\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script ftfy.exe is installed in 'C:\\Users\\u651278\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install clean-text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erlkönig\n",
      "J.W. Goethe\n",
      "\n",
      "Wer reitet so spät durch Nacht und Wind?\n",
      "Es ist der Vater mit seinem Kind;\n",
      "Er hat den Knaben wohl in dem Arm,\n",
      "Er faßt ihn sicher, er hält ihn warm.\n",
      "  \t \n",
      "«Mein Sohn, was birgst du so bang dein Gesicht?» \n",
      "Siehst, Vater, du den Erlkönig nicht?\n",
      "Den Erlenkönig mit Kron und Schweif? \n",
      "«Mein Sohn, es ist ein Nebelstreif.» \n",
      "  \t \n",
      "«Du liebes Kind, komm, geh mit mir!\n",
      "Gar schöne Spiele spiel' ich mit dir;\n",
      "Manch bunte Blumen sind an dem Strand,\n",
      "Meine Mutter hat manch gülden Gewand.»\n",
      "  \t \n",
      "Mein Vater, mein Vater, und hörest du nicht,\n",
      "Was Erlenkönig mir leise verspricht? \n",
      "«Sei ruhig, bleibe ruhig, mein Kind;\n",
      "In dürren Blättern säuselt der Wind.»\n",
      "  \t \n",
      "«Willst, feiner Knabe, du mit mir gehn?\n",
      "Meine Töchter sollen dich warten schön;\n",
      "Meine Töchter führen den nächtlichen Reihn,\n",
      "Und wiegen und tanzen und singen dich ein.»\n",
      "  \t \n",
      "Mein Vater, mein Vater, und siehst du nicht dort\n",
      "Erlkönigs Töchter am düstern Ort? \n",
      "«Mein Sohn, mein Sohn, ich seh es genau:\n",
      "Es scheinen die alten Weiden so grau.»\n",
      "  \t \n",
      "«Ich liebe dich, mich reizt deine schöne Gestalt;\n",
      "Und bist du nicht willig, so brauch ich Gewalt.»\n",
      "Mein Vater, mein Vater, jetzt faßt er mich an!\n",
      "Erlkönig hat mir ein Leids getan! \n",
      "  \t \n",
      "Dem Vater grausets, er reitet geschwind,\n",
      "Er hält in Armen das ächzende Kind,\n",
      "Erreicht den Hof mit Mühe und Not;\n",
      "In seinen Armen das Kind war tot.\n"
     ]
    }
   ],
   "source": [
    "# Try reading the file with a different encoding (ISO-8859-1 or Windows-1252)\n",
    "file_path = r\"C:/Users/u651278/Downloads/text files/pc_erlkonig.txt\"\n",
    "\n",
    "# Attempt to open the file using 'ISO-8859-1' encoding\n",
    "with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erlknig\n",
      "J.W. Goethe\n",
      "\n",
      "Wer reitet so spt durch Nacht und Wind?\n",
      "Es ist der Vater mit seinem Kind;\n",
      "Er hat den Knaben wohl in dem Arm,\n",
      "Er fat ihn sicher, er hlt ihn warm.\n",
      "  \t \n",
      "Mein Sohn, was birgst du so bang dein Gesicht? \n",
      "Siehst, Vater, du den Erlknig nicht?\n",
      "Den Erlenknig mit Kron und Schweif? \n",
      "Mein Sohn, es ist ein Nebelstreif. \n",
      "  \t \n",
      "Du liebes Kind, komm, geh mit mir!\n",
      "Gar schne Spiele spiel' ich mit dir;\n",
      "Manch bunte Blumen sind an dem Strand,\n",
      "Meine Mutter hat manch glden Gewand.\n",
      "  \t \n",
      "Mein Vater, mein Vater, und hrest du nicht,\n",
      "Was Erlenknig mir leise verspricht? \n",
      "Sei ruhig, bleibe ruhig, mein Kind;\n",
      "In drren Blttern suselt der Wind.\n",
      "  \t \n",
      "Willst, feiner Knabe, du mit mir gehn?\n",
      "Meine Tchter sollen dich warten schn;\n",
      "Meine Tchter fhren den nchtlichen Reihn,\n",
      "Und wiegen und tanzen und singen dich ein.\n",
      "  \t \n",
      "Mein Vater, mein Vater, und siehst du nicht dort\n",
      "Erlknigs Tchter am dstern Ort? \n",
      "Mein Sohn, mein Sohn, ich seh es genau:\n",
      "Es scheinen die alten Weiden so grau.\n",
      "  \t \n",
      "Ich liebe dich, mich reizt deine schne Gestalt;\n",
      "Und bist du nicht willig, so brauch ich Gewalt.\n",
      "Mein Vater, mein Vater, jetzt fat er mich an!\n",
      "Erlknig hat mir ein Leids getan! \n",
      "  \t \n",
      "Dem Vater grausets, er reitet geschwind,\n",
      "Er hlt in Armen das chzende Kind,\n",
      "Erreicht den Hof mit Mhe und Not;\n",
      "In seinen Armen das Kind war tot.\n"
     ]
    }
   ],
   "source": [
    "# Path to your text file\n",
    "file_path = r\"C:/Users/u651278/Downloads/text files/pc_erlkonig.txt\"\n",
    "\n",
    "# Read the file with the correct encoding (if necessary)\n",
    "with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Remove the unwanted character \"\" from the text\n",
    "cleaned_text = text.replace('', '')  # Replace the character with an empty string\n",
    "\n",
    "# Print the cleaned text (optional)\n",
    "print(cleaned_text)\n",
    "\n",
    "# Optionally, save the cleaned content back to a new file\n",
    "cleaned_file_path = \"text files/erlkonig_cleaned.txt\"\n",
    "with open(cleaned_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text has been written back to C:/Users/u651278/Downloads/text files/pc_erlkonig.txt\n"
     ]
    }
   ],
   "source": [
    "# Path to your text file\n",
    "file_path = r\"C:/Users/u651278/Downloads/text files/pc_erlkonig.txt\"\n",
    "\n",
    "# Read the file with the correct encoding (use 'utf-8' or the encoding that works)\n",
    "with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Remove the unwanted character \"\" from the text\n",
    "cleaned_text = text.replace('', '')  # Replace the character with an empty string\n",
    "\n",
    "# Write the cleaned text back into the same file\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "print(f\"Cleaned text has been written back to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We succesfully corected the file with many typos and special characters. Now we are going to re run our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9390\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       809\n",
      "           1       0.67      0.15      0.25        13\n",
      "           2       1.00      0.13      0.23        47\n",
      "\n",
      "    accuracy                           0.94       869\n",
      "   macro avg       0.87      0.43      0.48       869\n",
      "weighted avg       0.94      0.94      0.92       869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import SGDClassifier  # Import SGDClassifier for gradient descent optimization\n",
    "\n",
    "# Folder is already open in VSCode, so use the relative path\n",
    "folder_path = r\"C:/Users/u651278/Downloads/text files\"  # Use relative path if folder is within the workspace\n",
    "\n",
    "def read_text_files_from_folder(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # Only consider .txt files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Use 'ignore' or 'replace' to handle invalid characters\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    texts.append(file.read())  # Add the content of each file\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Error: {file_path} not found.\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Read the content of all text files in the folder\n",
    "texts = read_text_files_from_folder(folder_path)\n",
    "\n",
    "# Prepare the features and labels\n",
    "window_size = 5  # Number of previous characters used to predict the next one\n",
    "\n",
    "X = []  # Features (text snippets)\n",
    "y = []  # Labels (comma or capital letter)\n",
    "\n",
    "# Function to check if a character is a comma or a capital letter\n",
    "def is_comma_or_capital(char):\n",
    "    if char == ',':\n",
    "        return 1  # Comma\n",
    "    elif char.isupper():\n",
    "        return 2  # Capital letter\n",
    "    return 0  # Neither comma nor capital letter\n",
    "\n",
    "# Loop through the content of all files and prepare data\n",
    "for text in texts:\n",
    "    for i in range(len(text) - window_size):\n",
    "        # Create a feature vector for the previous 'window_size' characters\n",
    "        window = text[i:i + window_size]\n",
    "        \n",
    "        # Create the target variable: what comes next (comma or capital letter)\n",
    "        next_char = text[i + window_size]\n",
    "        \n",
    "        # Add the feature (text snippet)\n",
    "        X.append(window)  # Use text snippets as features\n",
    "        \n",
    "        # Add the label for the next character (comma or capital letter)\n",
    "        y.append(is_comma_or_capital(next_char))\n",
    "\n",
    "# Convert to numpy arrays for model compatibility\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Use TF-IDF to vectorize the text snippets (features)\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))  # Using character bigrams and trigrams\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use SGDClassifier for gradient descent optimization\n",
    "# We can specify loss='log' for logistic regression loss function\n",
    "model = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9494\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97       809\n",
      "           1       0.70      0.54      0.61        13\n",
      "           2       0.70      0.40      0.51        47\n",
      "\n",
      "    accuracy                           0.95       869\n",
      "   macro avg       0.79      0.64      0.70       869\n",
      "weighted avg       0.94      0.95      0.94       869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use CountVectorizer instead of TF-IDF\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))  # Bigrams and trigrams\n",
    "X_count = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_count, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use SGDClassifier for gradient descent optimization\n",
    "model = SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print classification report for detailed metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.0001, 'eta0': 0.01, 'max_iter': 2000}\n",
      "Best Cross-validation Accuracy: 0.9494\n",
      "Test Accuracy: 0.9540\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter': [1000, 2000],\n",
    "    'eta0': [0.001, 0.01, 0.1],  # Learning rates\n",
    "    'alpha': [0.0001, 0.001, 0.01],  # Regularization strength\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SGDClassifier(loss='log_loss'), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best estimator to predict and evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, implementing 2 different models after cleaning an training through our 4 .txt files and we aquired 2 higher accuracy results, .9494 and .9540. \n",
    "The second model showed the higher score for natural languge processing by implementing stochastic gradient descent models for optimization and the closest result to 1!\n",
    "\n",
    "If you made it until here, thank you reading me :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
